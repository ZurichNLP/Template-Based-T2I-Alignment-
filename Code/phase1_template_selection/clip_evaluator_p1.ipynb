{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating samples: 100%|██████████| 100/100 [16:00<00:00,  9.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEMPLATE EVALUATION REPORT ===\n",
      "\n",
      "Best Overall Template: Basic Object Focus\n",
      "Composite Score: 5.3112\n",
      "\n",
      "Detailed Metrics:\n",
      "-----------------\n",
      "\n",
      "Basic Object Focus:\n",
      "  Mean CLIP Score: 0.2108\n",
      "  Standard Deviation: 0.0400\n",
      "  Success Rate: 56.6%\n",
      "  Times Best: 25\n",
      "  Times Worst: 11\n",
      "\n",
      "Contextual Scene:\n",
      "  Mean CLIP Score: 0.2099\n",
      "  Standard Deviation: 0.0439\n",
      "  Success Rate: 52.0%\n",
      "  Times Best: 27\n",
      "  Times Worst: 15\n",
      "\n",
      "Educational Layout:\n",
      "  Mean CLIP Score: 0.2024\n",
      "  Standard Deviation: 0.0497\n",
      "  Success Rate: 46.5%\n",
      "  Times Best: 17\n",
      "  Times Worst: 28\n",
      "\n",
      "Multi-Level Detail:\n",
      "  Mean CLIP Score: 0.2005\n",
      "  Standard Deviation: 0.0431\n",
      "  Success Rate: 48.5%\n",
      "  Times Best: 19\n",
      "  Times Worst: 13\n",
      "\n",
      "Grid Layout:\n",
      "  Mean CLIP Score: 0.1988\n",
      "  Standard Deviation: 0.0424\n",
      "  Success Rate: 44.8%\n",
      "  Times Best: 11\n",
      "  Times Worst: 32\n",
      "\n",
      "Analysis files saved in: ..\\output_files\\clip_analysis\n",
      "\n",
      "Evaluation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class CLIPEvaluator:\n",
    "    def __init__(self):\n",
    "        # Initialize paths using your existing structure\n",
    "        self.base_path = os.path.join('..', 'images')\n",
    "        self.results_path = os.path.join('..', 'output_files')\n",
    "        self.prompts_file = os.path.join(self.results_path, 'generated_prompts.json')\n",
    "        \n",
    "        # Create results directory for analysis outputs\n",
    "        self.analysis_path = os.path.join(self.results_path, 'clip_analysis')\n",
    "        os.makedirs(self.analysis_path, exist_ok=True)\n",
    "        \n",
    "        # Initialize CLIP model and attributes\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        model, preprocess = clip.load('ViT-L/14@336px', device=self.device)\n",
    "        self.model = model\n",
    "        self.preprocess = preprocess\n",
    "        \n",
    "        # Template names\n",
    "        self.templates = [\n",
    "            \"Basic Object Focus\",\n",
    "            \"Contextual Scene\",\n",
    "            \"Educational Layout\", \n",
    "            \"Multi-Level Detail\",\n",
    "            \"Grid Layout\"\n",
    "        ]\n",
    "        \n",
    "    def load_prompts(self) -> List[Dict]:\n",
    "        \"\"\"Load the generated prompts from JSON file\"\"\"\n",
    "        try:\n",
    "            with open(self.prompts_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading prompts file: {str(e)}\")\n",
    "            return []\n",
    "        \n",
    "        # Create results directory for analysis outputs\n",
    "        self.analysis_path = os.path.join(self.results_path, 'clip_analysis')\n",
    "        os.makedirs(self.analysis_path, exist_ok=True)\n",
    "        \n",
    "        # Create results directory for analysis outputs\n",
    "        self.analysis_path = os.path.join(self.results_path, 'clip_analysis')\n",
    "        os.makedirs(self.analysis_path, exist_ok=True)\n",
    "        \n",
    "        # Load CLIP model\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        model, preprocess = clip.load('ViT-L/14@336px', device=self.device)\n",
    "        self.model = model\n",
    "        self.preprocess = preprocess\n",
    "        \n",
    "        self.templates = [\n",
    "            \"Basic Object Focus\",\n",
    "            \"Contextual Scene\",\n",
    "            \"Educational Layout\", \n",
    "            \"Multi-Level Detail\",\n",
    "            \"Grid Layout\"\n",
    "        ]\n",
    "\n",
    "    def analyze_template_performance(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Comprehensive template analysis considering multiple metrics:\n",
    "        1. Mean CLIP score - Overall performance\n",
    "        2. Standard deviation - Consistency\n",
    "        3. Success rate - Reliability\n",
    "        4. Top performance count - How often each template is the best\n",
    "        5. Bottom performance count - How often each template is the worst\n",
    "        \"\"\"\n",
    "        # Calculate basic statistics\n",
    "        stats = df.groupby('template')['clip_score'].agg([\n",
    "            ('mean_score', 'mean'),\n",
    "            ('std_dev', 'std'),\n",
    "            ('count', 'count')\n",
    "        ])\n",
    "        \n",
    "        # Calculate success rate (percentage of images above mean CLIP score)\n",
    "        mean_clip = df['clip_score'].mean()\n",
    "        success_rates = []\n",
    "        \n",
    "        for template in self.templates:\n",
    "            template_scores = df[df['template'] == template]['clip_score']\n",
    "            success_rate = (template_scores > mean_clip).mean() * 100\n",
    "            success_rates.append(success_rate)\n",
    "        \n",
    "        stats['success_rate'] = success_rates\n",
    "        \n",
    "        # Count times each template performs best/worst per sample\n",
    "        best_counts = []\n",
    "        worst_counts = []\n",
    "        \n",
    "        for _, sample_group in df.groupby('sample_id'):\n",
    "            best_template = sample_group.loc[sample_group['clip_score'].idxmax(), 'template']\n",
    "            worst_template = sample_group.loc[sample_group['clip_score'].idxmin(), 'template']\n",
    "            \n",
    "            for template in self.templates:\n",
    "                best_counts.append((template, template == best_template))\n",
    "                worst_counts.append((template, template == worst_template))\n",
    "        \n",
    "        best_df = pd.DataFrame(best_counts, columns=['template', 'is_best'])\n",
    "        worst_df = pd.DataFrame(worst_counts, columns=['template', 'is_worst'])\n",
    "        \n",
    "        stats['times_best'] = best_df.groupby('template')['is_best'].sum()\n",
    "        stats['times_worst'] = worst_df.groupby('template')['is_worst'].sum()\n",
    "        \n",
    "        # Calculate a composite score (you can adjust weights based on importance)\n",
    "        stats['composite_score'] = (\n",
    "            0.4 * stats['mean_score'] +  # Overall performance\n",
    "            0.2 * (1 / stats['std_dev']) +  # Consistency (inverse of std dev)\n",
    "            0.2 * (stats['success_rate'] / 100) +  # Success rate\n",
    "            0.1 * (stats['times_best'] / len(df['sample_id'].unique())) +  # Best performance rate\n",
    "            0.1 * (1 - stats['times_worst'] / len(df['sample_id'].unique()))  # Inverse of worst performance rate\n",
    "        )\n",
    "        \n",
    "        return stats.round(4)\n",
    "\n",
    "    def visualize_results(self, df: pd.DataFrame, stats: pd.DataFrame):\n",
    "        \"\"\"Create comprehensive visualizations for the analysis\"\"\"\n",
    "        # 1. Template Performance Distribution (Box Plot)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(x='template', y='clip_score', data=df)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('CLIP Score Distribution by Template')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.analysis_path, 'template_distribution.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # 2. Success Rate Comparison (Bar Plot)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        stats['success_rate'].plot(kind='bar')\n",
    "        plt.title('Template Success Rates')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.analysis_path, 'success_rates.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # 3. Composite Score Visualization\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        stats['composite_score'].plot(kind='bar')\n",
    "        plt.title('Template Composite Scores')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.analysis_path, 'composite_scores.png'))\n",
    "        plt.close()\n",
    "\n",
    "    def get_clip_score(self, image_path: str, text: str) -> float:\n",
    "        \"\"\"Calculate CLIP score for a single image-text pair\"\"\"\n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            image = Image.open(image_path)\n",
    "            image_input = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Prepare text\n",
    "            text_input = clip.tokenize([text]).to(self.device)\n",
    "            \n",
    "            # Generate embeddings\n",
    "            with torch.no_grad():\n",
    "                image_features = self.model.encode_image(image_input)\n",
    "                text_features = self.model.encode_text(text_input)\n",
    "            \n",
    "            # Normalize and calculate similarity\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            return torch.matmul(image_features, text_features.T).item()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {str(e)}\")\n",
    "            return 0.0\n",
    "\n",
    "    def evaluate_all_images(self):\n",
    "        \"\"\"Evaluate all images and perform comprehensive analysis\"\"\"\n",
    "        results = []\n",
    "        prompts_data = self.load_prompts()\n",
    "        \n",
    "        # Process each sample\n",
    "        for sample in tqdm(prompts_data, desc=\"Evaluating samples\"):\n",
    "            sample_id = sample['id']\n",
    "            simplified_text = sample['simplified_text']\n",
    "            \n",
    "            for template_prompt in sample['template_prompts']:\n",
    "                template_name = template_prompt['template_name']\n",
    "                \n",
    "                # Check both image organizations\n",
    "                image_name = f\"{template_name.lower().replace(' ', '_')}.png\"\n",
    "                sample_path = os.path.join(self.base_path, 'by_sample', sample_id, image_name)\n",
    "                \n",
    "                if os.path.exists(sample_path):\n",
    "                    clip_score = self.get_clip_score(sample_path, simplified_text)\n",
    "                    results.append({\n",
    "                        'sample_id': sample_id,\n",
    "                        'template': template_name,\n",
    "                        'clip_score': clip_score\n",
    "                    })\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Perform comprehensive analysis\n",
    "        stats = self.analyze_template_performance(df)\n",
    "        \n",
    "        # Create visualizations\n",
    "        self.visualize_results(df, stats)\n",
    "        \n",
    "        # Save detailed results\n",
    "        df.to_csv(os.path.join(self.analysis_path, 'clip_scores_detailed.csv'), index=False)\n",
    "        stats.to_csv(os.path.join(self.analysis_path, 'clip_scores_analysis.csv'))\n",
    "        \n",
    "        # Print comprehensive report\n",
    "        self.print_analysis_report(stats)\n",
    "        \n",
    "        return df, stats\n",
    "\n",
    "    def print_analysis_report(self, stats: pd.DataFrame):\n",
    "        \"\"\"Print a detailed analysis report\"\"\"\n",
    "        print(\"\\n=== TEMPLATE EVALUATION REPORT ===\\n\")\n",
    "        \n",
    "        # Overall best template\n",
    "        best_template = stats['composite_score'].idxmax()\n",
    "        print(f\"Best Overall Template: {best_template}\")\n",
    "        print(f\"Composite Score: {stats.loc[best_template, 'composite_score']:.4f}\\n\")\n",
    "        \n",
    "        print(\"Detailed Metrics:\")\n",
    "        print(\"-----------------\")\n",
    "        for template in self.templates:\n",
    "            print(f\"\\n{template}:\")\n",
    "            print(f\"  Mean CLIP Score: {stats.loc[template, 'mean_score']:.4f}\")\n",
    "            print(f\"  Standard Deviation: {stats.loc[template, 'std_dev']:.4f}\")\n",
    "            print(f\"  Success Rate: {stats.loc[template, 'success_rate']:.1f}%\")\n",
    "            print(f\"  Times Best: {stats.loc[template, 'times_best']}\")\n",
    "            print(f\"  Times Worst: {stats.loc[template, 'times_worst']}\")\n",
    "        \n",
    "        print(\"\\nAnalysis files saved in:\", self.analysis_path)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        evaluator = CLIPEvaluator()\n",
    "        results_df, stats = evaluator.evaluate_all_images()\n",
    "        print(\"\\nEvaluation completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the data\n",
    "data = pd.DataFrame({\n",
    "    'Template': ['Basic Object Focus', 'Contextual Scene', 'Educational Layout', 'Grid Layout', 'Multi-Level Detail'],\n",
    "    'CLIP Score': [0.2108, 0.2099, 0.2024, 0.1988, 0.2005],\n",
    "    'Std Dev': [0.04, 0.0439, 0.0497, 0.0424, 0.0431],\n",
    "    'Success Rate': [56.5657, 52.0408, 46.4646, 44.7917, 48.4536],\n",
    "    'Times Best': [25, 27, 17, 11, 19],\n",
    "    'Times Worst': [11, 15, 28, 32, 13],\n",
    "    'Composite Score': [5.3112, 4.8541, 4.2894, 4.9646, 4.9284]\n",
    "})\n",
    "\n",
    "# Set basic style parameters\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Figure 1: CLIP Scores with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.errorbar(data['Template'], data['CLIP Score'], \n",
    "            yerr=data['Std Dev'], \n",
    "            fmt='o', capsize=5, \n",
    "            capthick=1.5, elinewidth=1.5,\n",
    "            color='royalblue',\n",
    "            ecolor='darkblue')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('CLIP Score')\n",
    "plt.title('CLIP Scores by Template with Standard Deviation')\n",
    "plt.tight_layout()\n",
    "plt.savefig('clip_scores.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Figure 2: Success Rates\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "bars = plt.bar(data['Template'], data['Success Rate'], color='royalblue', alpha=0.7)\n",
    "plt.axhline(y=50, color='red', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Success Rate (%)')\n",
    "plt.title('Template Success Rates')\n",
    "# Add value labels on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}%',\n",
    "             ha='center', va='bottom')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('success_rates.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Figure 3: Best/Worst Analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "x = np.arange(len(data['Template']))\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, data['Times Best'], width, label='Times Best', color='forestgreen', alpha=0.6)\n",
    "plt.bar(x + width/2, data['Times Worst'], width, label='Times Worst', color='crimson', alpha=0.6)\n",
    "plt.xticks(x, data['Template'], rotation=45, ha='right')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Best vs Worst Performance by Template')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('best_worst.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Figure 4: Composite Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "bars = plt.bar(data['Template'], data['Composite Score'], color='royalblue', alpha=0.7)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Composite Score')\n",
    "plt.title('Overall Template Performance (Composite Score)')\n",
    "# Add value labels on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}',\n",
    "             ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.savefig('composite_scores.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
