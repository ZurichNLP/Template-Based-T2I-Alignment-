{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the path to your images directory\n",
    "images_dir = Path(\"C:/Users/SouayedBelkiss/OneDrive - gae/Desktop/Thesis/Label_Studio/all_images\")\n",
    "\n",
    "# Get all image files\n",
    "image_files = [f for f in os.listdir(images_dir) if f.lower().endswith('.png')]\n",
    "print(f\"Found {len(image_files)} images\")\n",
    "\n",
    "# Shuffle the list\n",
    "random.seed(42)  # For reproducibility\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# Create a DataFrame with the shuffled list\n",
    "df = pd.DataFrame({'filename': image_files})\n",
    "\n",
    "# Save to CSV in your code directory\n",
    "output_dir = Path(\"C:/Users/SouayedBelkiss/OneDrive - gae/Desktop/Thesis/Label_Studio/code\")\n",
    "shuffled_list_file = output_dir / \"shuffled_images.csv\"\n",
    "df.to_csv(shuffled_list_file, index=False)\n",
    "\n",
    "# Display the first 10 shuffled images\n",
    "print(f\"\\nShuffled list saved to {shuffled_list_file}\")\n",
    "print(\"\\nFirst 10 images in shuffled list:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found image directory: C:\\Users\\SouayedBelkiss\\OneDrive - gae\\Desktop\\Thesis\\phase3_label_studio\\shuffled_images\n",
      "Found 3935 PNG images\n",
      "\n",
      "Sample filenames:\n",
      "  0001_simpa_simpa_173_artistic.png\n",
      "  0002_asset_asset_125_technical.png\n",
      "  0003_simpa_simpa_239_cartoon.png\n",
      "  0004_simpa_simpa_398_cartoon.png\n",
      "  0005_asset_asset_137_realistic.png\n",
      "\n",
      "Detailed analysis of first filename:\n",
      "  Filename: 0001_simpa_simpa_173_artistic.png\n",
      "  Parts after splitting by underscore: ['0001', 'simpa', 'simpa', '173', 'artistic.png']\n",
      "  Last part: artistic.png\n",
      "  Style (last part without extension): artistic\n",
      "\n",
      "Found 10 potential styles in first 100 images:\n",
      "  artistic: 16 files\n",
      "  technical: 14 files\n",
      "  cartoon: 8 files\n",
      "  realistic: 11 files\n",
      "  digital art: 8 files\n",
      "  storybook: 7 files\n",
      "  3d rendered: 12 files\n",
      "  geometric: 7 files\n",
      "  retro: 7 files\n",
      "  minimalistic: 10 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Use the correct path to your shuffled images\n",
    "shuffled_images_dir = Path(\"C:/Users/SouayedBelkiss/OneDrive - gae/Desktop/Thesis/phase3_label_studio/shuffled_images\")\n",
    "\n",
    "# Check if directory exists\n",
    "if not os.path.exists(shuffled_images_dir):\n",
    "    print(f\"Directory does not exist: {shuffled_images_dir}\")\n",
    "else:\n",
    "    print(f\"Found image directory: {shuffled_images_dir}\")\n",
    "    \n",
    "    # Get all image files (limit to first 20 to avoid memory issues)\n",
    "    image_files = [f for f in os.listdir(shuffled_images_dir) if f.lower().endswith('.png')]\n",
    "    print(f\"Found {len(image_files)} PNG images\")\n",
    "    \n",
    "    # Print sample filenames\n",
    "    print(\"\\nSample filenames:\")\n",
    "    for filename in image_files[:5]:\n",
    "        print(f\"  {filename}\")\n",
    "    \n",
    "    # Analyze one filename in detail\n",
    "    if image_files:\n",
    "        print(\"\\nDetailed analysis of first filename:\")\n",
    "        filename = image_files[0]\n",
    "        print(f\"  Filename: {filename}\")\n",
    "        parts = filename.split('_')\n",
    "        print(f\"  Parts after splitting by underscore: {parts}\")\n",
    "        \n",
    "        # Extract potential style (last part before extension)\n",
    "        if len(parts) > 1:\n",
    "            last_part = parts[-1]\n",
    "            print(f\"  Last part: {last_part}\")\n",
    "            style = last_part.split('.')[0]\n",
    "            print(f\"  Style (last part without extension): {style}\")\n",
    "            \n",
    "        # Find all unique styles (analyzing more files)\n",
    "        styles = {}\n",
    "        for i, filename in enumerate(image_files):\n",
    "            if i >= 100:  # Limit to first 100 files to avoid memory issues\n",
    "                break\n",
    "                \n",
    "            parts = filename.split('_')\n",
    "            if len(parts) > 1:\n",
    "                style = parts[-1].split('.')[0]\n",
    "                styles[style] = styles.get(style, 0) + 1\n",
    "        \n",
    "        print(f\"\\nFound {len(styles)} potential styles in first 100 images:\")\n",
    "        for style, count in styles.items():\n",
    "            print(f\"  {style}: {count} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the 2000 set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3935 source images\n",
      "Found 400 unique base images\n",
      "Found 362 base images with all 10 style variations\n",
      "Selected 200 base images for our 2000_set\n",
      "Copied 2000 images to C:\\Users\\SouayedBelkiss\\OneDrive - gae\\Desktop\\Thesis\\phase3_label_studio\\2000_set\n",
      "Saved mapping to C:\\Users\\SouayedBelkiss\\OneDrive - gae\\Desktop\\Thesis\\phase3_label_studio\\code\\2000_set_mapping.csv\n",
      "\n",
      "Style distribution in selected set:\n",
      "style\n",
      "artistic        200\n",
      "digital art     200\n",
      "realistic       200\n",
      "geometric       200\n",
      "storybook       200\n",
      "retro           200\n",
      "technical       200\n",
      "cartoon         200\n",
      "3d rendered     200\n",
      "minimalistic    200\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define paths - update these to match your environment\n",
    "SOURCE_DIR = Path(\"C:/Users/SouayedBelkiss/OneDrive - gae/Desktop/Thesis/phase3_label_studio/shuffled_images\")\n",
    "TARGET_DIR = Path(\"C:/Users/SouayedBelkiss/OneDrive - gae/Desktop/Thesis/phase3_label_studio/2000_set\")\n",
    "\n",
    "# Create target directory if it doesn't exist\n",
    "os.makedirs(TARGET_DIR, exist_ok=True)\n",
    "\n",
    "# Get all image files from source directory\n",
    "image_files = [f for f in os.listdir(SOURCE_DIR) if f.lower().endswith('.png')]\n",
    "print(f\"Found {len(image_files)} source images\")\n",
    "\n",
    "# Group images by their dataset_ID combination\n",
    "image_groups = defaultdict(list)\n",
    "\n",
    "for img_file in image_files:\n",
    "    # Parse filename to extract components\n",
    "    # Format: ####_dataset_dataset_ID_style.png\n",
    "    parts = img_file.split('_')\n",
    "    \n",
    "    if len(parts) >= 4:  # Make sure there are enough parts\n",
    "        # The dataset_ID key will be dataset_dataset_ID\n",
    "        dataset_id_key = f\"{parts[1]}_{parts[2]}_{parts[3]}\"\n",
    "        image_groups[dataset_id_key].append(img_file)\n",
    "\n",
    "# Check how many unique base images we have\n",
    "print(f\"Found {len(image_groups)} unique base images\")\n",
    "\n",
    "# Verify that each base image has 10 style variations\n",
    "complete_groups = {k: v for k, v in image_groups.items() if len(v) == 10}\n",
    "print(f\"Found {len(complete_groups)} base images with all 10 style variations\")\n",
    "\n",
    "# Select 200 complete groups (base images with 10 styles each)\n",
    "selected_groups = list(complete_groups.keys())[:200]\n",
    "print(f\"Selected {len(selected_groups)} base images for our 2000_set\")\n",
    "\n",
    "# Create a mapping file to keep track of selected images\n",
    "selected_image_mapping = []\n",
    "\n",
    "# Copy selected images to target directory with error handling\n",
    "copied_count = 0\n",
    "errors = []\n",
    "\n",
    "for group_key in selected_groups:\n",
    "    image_files = image_groups[group_key]\n",
    "    for img_file in image_files:\n",
    "        source_path = SOURCE_DIR / img_file\n",
    "        target_path = TARGET_DIR / img_file\n",
    "        \n",
    "        try:\n",
    "            # Copy the file\n",
    "            shutil.copy2(str(source_path), str(target_path))  # Convert Path to string\n",
    "            copied_count += 1\n",
    "            \n",
    "            # Add to mapping\n",
    "            selected_image_mapping.append({\n",
    "                'original_filename': img_file,\n",
    "                'base_image_id': group_key,\n",
    "                'style': img_file.split('_')[-1].split('.')[0]  # Extract style\n",
    "            })\n",
    "        except Exception as e:\n",
    "            errors.append(f\"Error copying {img_file}: {str(e)}\")\n",
    "\n",
    "print(f\"Copied {copied_count} images to {TARGET_DIR}\")\n",
    "if errors:\n",
    "    print(f\"Encountered {len(errors)} errors:\")\n",
    "    for error in errors[:5]:  # Show just the first 5 errors\n",
    "        print(f\"  {error}\")\n",
    "\n",
    "# If we have successful copies, save the mapping\n",
    "if copied_count > 0:\n",
    "    # Save the mapping to a CSV file\n",
    "    mapping_df = pd.DataFrame(selected_image_mapping)\n",
    "    mapping_path = Path(\"C:/Users/SouayedBelkiss/OneDrive - gae/Desktop/Thesis/phase3_label_studio/code/2000_set_mapping.csv\")\n",
    "    mapping_df.to_csv(mapping_path, index=False)\n",
    "    print(f\"Saved mapping to {mapping_path}\")\n",
    "\n",
    "    # Count styles in selected set\n",
    "    style_counts = mapping_df['style'].value_counts()\n",
    "    print(\"\\nStyle distribution in selected set:\")\n",
    "    print(style_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a verification code to check if all 10 styles for a specific base image (like simpa_173) are in the 2000_set folder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found target directory: C:\\Users\\SouayedBelkiss\\OneDrive - gae\\Desktop\\Thesis\\phase3_label_studio\\2000_set\n",
      "Found 2000 images in the 2000_set folder\n",
      "\n",
      "Checking styles for base image: wikipedia_wikipedia_395\n",
      "Found 10 style variations:\n",
      "  - digital art\n",
      "  - technical\n",
      "  - 3d rendered\n",
      "  - minimalistic\n",
      "  - cartoon\n",
      "  - retro\n",
      "  - realistic\n",
      "  - geometric\n",
      "  - artistic\n",
      "  - storybook\n",
      "\n",
      "Checking a few random base images:\n",
      "\n",
      "Base image 1: wikipedia_wikipedia_197\n",
      "  Found 10 style variations\n",
      "\n",
      "Base image 2: onestopenglish_onestop_058\n",
      "  Found 10 style variations\n",
      "\n",
      "Base image 3: simpa_simpa_122\n",
      "  Found 10 style variations\n",
      "\n",
      "Base image 4: simpa_simpa_422\n",
      "  Found 10 style variations\n",
      "\n",
      "Base image 5: onestopenglish_onestop_235\n",
      "  Found 10 style variations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the path to the 2000_set directory\n",
    "target_dir = Path(\"C:/Users/SouayedBelkiss/OneDrive - gae/Desktop/Thesis/phase3_label_studio/2000_set\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(target_dir):\n",
    "    print(f\"Directory does not exist: {target_dir}\")\n",
    "else:\n",
    "    print(f\"Found target directory: {target_dir}\")\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(target_dir) if f.lower().endswith('.png')]\n",
    "    print(f\"Found {len(image_files)} images in the 2000_set folder\")\n",
    "    \n",
    "    # Check for a specific base image (e.g., simpa_173)\n",
    "    base_image_to_check = \"wikipedia_wikipedia_395\"\n",
    "    matching_files = [f for f in image_files if base_image_to_check in f]\n",
    "    print(f\"\\nChecking styles for base image: {base_image_to_check}\")\n",
    "    if matching_files:\n",
    "        print(f\"Found {len(matching_files)} style variations:\")\n",
    "        for file in matching_files:\n",
    "            # Extract the style from the filename\n",
    "            style = file.split('_')[-1].split('.')[0]\n",
    "            print(f\"  - {style}\")\n",
    "    else:\n",
    "        print(f\"No images found for base image: {base_image_to_check}\")\n",
    "        \n",
    "    # Let's check a few random base images\n",
    "    print(\"\\nChecking a few random base images:\")\n",
    "    \n",
    "    # Extract all unique base image IDs\n",
    "    base_image_ids = set()\n",
    "    for img_file in image_files:\n",
    "        parts = img_file.split('_')\n",
    "        if len(parts) >= 4:\n",
    "            base_id = f\"{parts[1]}_{parts[2]}_{parts[3]}\"\n",
    "            base_image_ids.add(base_id)\n",
    "    \n",
    "    # Check the first 5 base images (or fewer if there are less than 5)\n",
    "    for i, base_id in enumerate(list(base_image_ids)[:5]):\n",
    "        matching_files = [f for f in image_files if base_id in f]\n",
    "        print(f\"\\nBase image {i+1}: {base_id}\")\n",
    "        print(f\"  Found {len(matching_files)} style variations\")\n",
    "        if len(matching_files) != 10:\n",
    "            print(\"  Styles found:\")\n",
    "            for file in matching_files:\n",
    "                style = file.split('_')[-1].split('.')[0]\n",
    "                print(f\"    - {style}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### renaming 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images in 2000_set\n",
      "Renamed and copied 2000 images to C:\\Users\\SouayedBelkiss\\OneDrive - gae\\Desktop\\Thesis\\phase3_label_studio\\renamed_images\n",
      "Created mapping file at C:\\Users\\SouayedBelkiss\\OneDrive - gae\\Desktop\\Thesis\\phase3_label_studio\\code\\renamed_images_mapping.csv\n",
      "\n",
      "Sample of mapping file (first 5 rows):\n",
      "  new_filename                   original_filename sequence_number dataset  \\\n",
      "0     0001.png   0001_simpa_simpa_173_artistic.png            0001   simpa   \n",
      "1     0002.png  0002_asset_asset_125_technical.png            0002   asset   \n",
      "2     0003.png    0003_simpa_simpa_239_cartoon.png            0003   simpa   \n",
      "3     0004.png    0004_simpa_simpa_398_cartoon.png            0004   simpa   \n",
      "4     0005.png  0005_asset_asset_137_realistic.png            0005   asset   \n",
      "\n",
      "  dataset_id      style       base_image  \n",
      "0        173   artistic  simpa_simpa_173  \n",
      "1        125  technical  asset_asset_125  \n",
      "2        239    cartoon  simpa_simpa_239  \n",
      "3        398    cartoon  simpa_simpa_398  \n",
      "4        137  realistic  asset_asset_137  \n",
      "\n",
      "No duplicate filenames found - everything looks good!\n",
      "\n",
      "Total unique base images: 200\n",
      "\n",
      "All base images have exactly 10 style variations - perfect!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "SOURCE_DIR = Path(\"C:/Users/SouayedBelkiss/OneDrive - gae/Desktop/Thesis/phase3_label_studio/2000_set\")\n",
    "TARGET_DIR = Path(\"C:/Users/SouayedBelkiss/OneDrive - gae/Desktop/Thesis/phase3_label_studio/2000_set_renamed\")\n",
    "MAPPING_FILE = Path(\"C:/Users/SouayedBelkiss/OneDrive - gae/Desktop/Thesis/phase3_label_studio/code/renamed_images_mapping.csv\")\n",
    "\n",
    "# Create target directory if it doesn't exist\n",
    "os.makedirs(TARGET_DIR, exist_ok=True)\n",
    "\n",
    "# Get all image files from source directory\n",
    "image_files = [f for f in os.listdir(SOURCE_DIR) if f.lower().endswith('.png')]\n",
    "print(f\"Found {len(image_files)} images in 2000_set\")\n",
    "\n",
    "# Create a list to store the mapping information\n",
    "mapping_data = []\n",
    "\n",
    "# Copy and rename the files with error handling\n",
    "copied_count = 0\n",
    "errors = []\n",
    "\n",
    "for img_file in image_files:\n",
    "    try:\n",
    "        # Extract the sequence number (####) from the original filename\n",
    "        sequence_number = img_file.split('_')[0]\n",
    "        \n",
    "        # Create new filename (just the sequence number with extension)\n",
    "        new_filename = f\"{sequence_number}.png\"\n",
    "        \n",
    "        # Define source and target paths\n",
    "        source_path = SOURCE_DIR / img_file\n",
    "        target_path = TARGET_DIR / new_filename\n",
    "        \n",
    "        # Copy the file with the new name\n",
    "        shutil.copy2(str(source_path), str(target_path))\n",
    "        copied_count += 1\n",
    "        \n",
    "        # Parse additional metadata from the filename\n",
    "        parts = img_file.split('_')\n",
    "        dataset = parts[1] if len(parts) > 1 else \"\"\n",
    "        dataset_id = parts[3] if len(parts) > 3 else \"\"\n",
    "        style = parts[4].split('.')[0] if len(parts) > 4 else \"\"\n",
    "        \n",
    "        # Add to mapping data\n",
    "        mapping_data.append({\n",
    "            'new_filename': new_filename,\n",
    "            'original_filename': img_file,\n",
    "            'sequence_number': sequence_number,\n",
    "            'dataset': dataset,\n",
    "            'dataset_id': dataset_id,\n",
    "            'style': style,\n",
    "            'base_image': f\"{dataset}_{dataset}_{dataset_id}\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        errors.append(f\"Error processing {img_file}: {str(e)}\")\n",
    "\n",
    "# Create DataFrame from mapping data\n",
    "mapping_df = pd.DataFrame(mapping_data)\n",
    "\n",
    "# Save mapping to CSV\n",
    "mapping_df.to_csv(MAPPING_FILE, index=False)\n",
    "\n",
    "# Verify the results\n",
    "print(f\"Renamed and copied {copied_count} images to {TARGET_DIR}\")\n",
    "print(f\"Created mapping file at {MAPPING_FILE}\")\n",
    "\n",
    "if errors:\n",
    "    print(f\"Encountered {len(errors)} errors:\")\n",
    "    for error in errors[:5]:  # Show just the first 5 errors\n",
    "        print(f\"  {error}\")\n",
    "\n",
    "# Show a sample of the mapping for verification\n",
    "print(\"\\nSample of mapping file (first 5 rows):\")\n",
    "print(mapping_df.head())\n",
    "\n",
    "# Check for any duplicate new filenames\n",
    "duplicate_check = mapping_df['new_filename'].duplicated()\n",
    "if duplicate_check.any():\n",
    "    print(f\"\\nWARNING: Found {duplicate_check.sum()} duplicate new filenames\")\n",
    "    print(mapping_df[duplicate_check])\n",
    "else:\n",
    "    print(\"\\nNo duplicate filenames found - everything looks good!\")\n",
    "\n",
    "# Count how many unique base images we have\n",
    "unique_base_images = mapping_df['base_image'].nunique()\n",
    "print(f\"\\nTotal unique base images: {unique_base_images}\")\n",
    "\n",
    "# Verify each base image has 10 styles\n",
    "base_image_counts = mapping_df['base_image'].value_counts()\n",
    "incomplete_base_images = base_image_counts[base_image_counts != 10]\n",
    "\n",
    "if len(incomplete_base_images) > 0:\n",
    "    print(f\"\\nFound {len(incomplete_base_images)} base images without exactly 10 styles:\")\n",
    "    print(incomplete_base_images)\n",
    "else:\n",
    "    print(\"\\nAll base images have exactly 10 style variations - perfect!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expert Distribution\n",
    "\n",
    "* First, we'll select 200 images for the IAA set from the renamed_images folder\n",
    "* Then, create the expert distribution structure with 4 folders named after each expert\n",
    "* Copy the IAA set to each expert's folder\n",
    "* Distribute 450 unique images to each expert's unique set folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded mapping file with 2000 entries\n",
      "Found 200 unique base images\n",
      "Selected 20 base images for IAA\n",
      "Distributing 180 non-IAA base images among 4 experts\n",
      "Martin Kappus: 45 base images assigned\n",
      "Luisa Carrer: 45 base images assigned\n",
      "Katrin Andermatt: 45 base images assigned\n",
      "Alexa Lintner: 45 base images assigned\n",
      "Copied 200 IAA images to the IAA directory and each expert's IAA folder\n",
      "Copied 450 unique images to Martin Kappus's unique folder\n",
      "Copied 450 unique images to Luisa Carrer's unique folder\n",
      "Copied 450 unique images to Katrin Andermatt's unique folder\n",
      "Copied 450 unique images to Alexa Lintner's unique folder\n",
      "Saved distribution mapping to C:\\Users\\SouayedBelkiss\\OneDrive - gae\\Desktop\\Thesis\\phase3_label_studio\\code\\expert_distribution_mapping.csv\n",
      "\n",
      "Verification:\n",
      "Total images in mapping: 2000\n",
      "IAA images: 200\n",
      "Non-IAA images: 1800\n",
      "\n",
      "Martin Kappus:\n",
      "  Assigned in mapping: 450\n",
      "  Files in IAA folder: 200\n",
      "  Files in unique folder: 450\n",
      "  Total files: 650\n",
      "\n",
      "Luisa Carrer:\n",
      "  Assigned in mapping: 450\n",
      "  Files in IAA folder: 200\n",
      "  Files in unique folder: 450\n",
      "  Total files: 650\n",
      "\n",
      "Katrin Andermatt:\n",
      "  Assigned in mapping: 450\n",
      "  Files in IAA folder: 200\n",
      "  Files in unique folder: 450\n",
      "  Total files: 650\n",
      "\n",
      "Alexa Lintner:\n",
      "  Assigned in mapping: 450\n",
      "  Files in IAA folder: 200\n",
      "  Files in unique folder: 450\n",
      "  Total files: 650\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "RENAMED_IMAGES_DIR = Path(\"C:/Users/SouayedBelkiss/OneDrive - gae/Desktop/Thesis/phase3_label_studio/renamed_images\")\n",
    "EXPERT_DISTRIBUTION_DIR = Path(\"C:/Users/SouayedBelkiss/OneDrive - gae/Desktop/Thesis/phase3_label_studio/Expert_distribution\")\n",
    "IAA_DIR = Path(\"C:/Users/SouayedBelkiss/OneDrive - gae/Desktop/Thesis/phase3_label_studio/IAA_set\")\n",
    "MAPPING_FILE = Path(\"C:/Users/SouayedBelkiss/OneDrive - gae/Desktop/Thesis/phase3_label_studio/code/renamed_images_mapping.csv\")\n",
    "DISTRIBUTION_MAPPING_FILE = Path(\"C:/Users/SouayedBelkiss/OneDrive - gae/Desktop/Thesis/phase3_label_studio/code/expert_distribution_mapping.csv\")\n",
    "\n",
    "# Expert names\n",
    "EXPERTS = [\n",
    "    \"Martin Kappus\",\n",
    "    \"Luisa Carrer\",\n",
    "    \"Katrin Andermatt\",\n",
    "    \"Alexa Lintner\"\n",
    "]\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(EXPERT_DISTRIBUTION_DIR, exist_ok=True)\n",
    "os.makedirs(IAA_DIR, exist_ok=True)\n",
    "\n",
    "for expert in EXPERTS:\n",
    "    expert_dir = EXPERT_DISTRIBUTION_DIR / expert\n",
    "    os.makedirs(expert_dir, exist_ok=True)\n",
    "    os.makedirs(expert_dir / \"IAA_set\", exist_ok=True)\n",
    "    os.makedirs(expert_dir / \"unique_set\", exist_ok=True)\n",
    "\n",
    "# Load the mapping file\n",
    "try:\n",
    "    mapping_df = pd.read_csv(MAPPING_FILE)\n",
    "    print(f\"Loaded mapping file with {len(mapping_df)} entries\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading mapping file: {str(e)}\")\n",
    "    exit(1)\n",
    "\n",
    "# Get all unique base images\n",
    "unique_base_images = mapping_df['base_image'].unique()\n",
    "print(f\"Found {len(unique_base_images)} unique base images\")\n",
    "\n",
    "# Select 20 base images for IAA (will result in 200 images with 10 styles each)\n",
    "random.seed(42)  # For reproducibility\n",
    "iaa_base_images = random.sample(list(unique_base_images), 20)\n",
    "print(f\"Selected {len(iaa_base_images)} base images for IAA\")\n",
    "\n",
    "# Get all images for the IAA set (the full 10 styles for each selected base image)\n",
    "iaa_mask = mapping_df['base_image'].isin(iaa_base_images)\n",
    "iaa_images_df = mapping_df[iaa_mask].copy()\n",
    "iaa_images_df['is_iaa'] = True\n",
    "\n",
    "# Mark the remaining images as non-IAA\n",
    "non_iaa_images_df = mapping_df[~iaa_mask].copy()\n",
    "non_iaa_images_df['is_iaa'] = False\n",
    "\n",
    "# Combine back into a single dataframe\n",
    "updated_mapping_df = pd.concat([iaa_images_df, non_iaa_images_df])\n",
    "\n",
    "# Distribute the non-IAA images evenly among the 4 experts\n",
    "non_iaa_base_images = list(set(non_iaa_images_df['base_image'].unique()))\n",
    "print(f\"Distributing {len(non_iaa_base_images)} non-IAA base images among {len(EXPERTS)} experts\")\n",
    "\n",
    "# Split the base images into 4 equal groups\n",
    "np.random.seed(42)  # For reproducibility\n",
    "np.random.shuffle(non_iaa_base_images)\n",
    "expert_base_images = np.array_split(non_iaa_base_images, len(EXPERTS))\n",
    "\n",
    "# Create a dictionary to track which expert gets which base images\n",
    "expert_assignments = {}\n",
    "for i, expert in enumerate(EXPERTS):\n",
    "    expert_assignments[expert] = list(expert_base_images[i])\n",
    "    print(f\"{expert}: {len(expert_assignments[expert])} base images assigned\")\n",
    "\n",
    "# Add expert assignment to the mapping dataframe\n",
    "updated_mapping_df['assigned_expert'] = None\n",
    "\n",
    "for expert, base_images in expert_assignments.items():\n",
    "    mask = updated_mapping_df['base_image'].isin(base_images)\n",
    "    updated_mapping_df.loc[mask, 'assigned_expert'] = expert\n",
    "\n",
    "# Copy IAA images to the IAA_DIR and to each expert's IAA folder\n",
    "for _, row in iaa_images_df.iterrows():\n",
    "    source_path = RENAMED_IMAGES_DIR / row['new_filename']\n",
    "    \n",
    "    # Add _IAA suffix to the filename for clarity\n",
    "    iaa_filename = row['new_filename'].replace('.png', '_IAA.png')\n",
    "    \n",
    "    # Copy to main IAA directory\n",
    "    target_path = IAA_DIR / iaa_filename\n",
    "    shutil.copy2(str(source_path), str(target_path))\n",
    "    \n",
    "    # Copy to each expert's IAA folder\n",
    "    for expert in EXPERTS:\n",
    "        expert_iaa_path = EXPERT_DISTRIBUTION_DIR / expert / \"IAA_set\" / iaa_filename\n",
    "        shutil.copy2(str(source_path), str(expert_iaa_path))\n",
    "\n",
    "print(f\"Copied {len(iaa_images_df)} IAA images to the IAA directory and each expert's IAA folder\")\n",
    "\n",
    "# Copy unique images to each expert's unique folder\n",
    "for expert in EXPERTS:\n",
    "    expert_images_df = updated_mapping_df[(updated_mapping_df['assigned_expert'] == expert) & (~updated_mapping_df['is_iaa'])]\n",
    "    \n",
    "    for _, row in expert_images_df.iterrows():\n",
    "        source_path = RENAMED_IMAGES_DIR / row['new_filename']\n",
    "        target_path = EXPERT_DISTRIBUTION_DIR / expert / \"unique_set\" / row['new_filename']\n",
    "        shutil.copy2(str(source_path), str(target_path))\n",
    "    \n",
    "    print(f\"Copied {len(expert_images_df)} unique images to {expert}'s unique folder\")\n",
    "\n",
    "# Save the updated mapping with IAA and expert assignments\n",
    "updated_mapping_df.to_csv(DISTRIBUTION_MAPPING_FILE, index=False)\n",
    "print(f\"Saved distribution mapping to {DISTRIBUTION_MAPPING_FILE}\")\n",
    "\n",
    "# Verification\n",
    "print(\"\\nVerification:\")\n",
    "print(f\"Total images in mapping: {len(updated_mapping_df)}\")\n",
    "print(f\"IAA images: {updated_mapping_df['is_iaa'].sum()}\")\n",
    "print(f\"Non-IAA images: {(~updated_mapping_df['is_iaa']).sum()}\")\n",
    "\n",
    "for expert in EXPERTS:\n",
    "    expert_count = (updated_mapping_df['assigned_expert'] == expert).sum()\n",
    "    expert_iaa_dir = EXPERT_DISTRIBUTION_DIR / expert / \"IAA_set\"\n",
    "    expert_unique_dir = EXPERT_DISTRIBUTION_DIR / expert / \"unique_set\"\n",
    "    \n",
    "    iaa_files = len([f for f in os.listdir(expert_iaa_dir) if f.lower().endswith('.png')])\n",
    "    unique_files = len([f for f in os.listdir(expert_unique_dir) if f.lower().endswith('.png')])\n",
    "    \n",
    "    print(f\"\\n{expert}:\")\n",
    "    print(f\"  Assigned in mapping: {expert_count}\")\n",
    "    print(f\"  Files in IAA folder: {iaa_files}\")\n",
    "    print(f\"  Files in unique folder: {unique_files}\")\n",
    "    print(f\"  Total files: {iaa_files + unique_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a focused verification code to confirm the distribution is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying image distribution across experts...\n",
      "Martin Kappus:\n",
      "  IAA images: 200/200 ✓\n",
      "  Unique images: 450/450 ✓\n",
      "Luisa Carrer:\n",
      "  IAA images: 200/200 ✓\n",
      "  Unique images: 450/450 ✓\n",
      "Katrin Andermatt:\n",
      "  IAA images: 200/200 ✓\n",
      "  Unique images: 450/450 ✓\n",
      "Alexa Lintner:\n",
      "  IAA images: 200/200 ✓\n",
      "  Unique images: 450/450 ✓\n",
      "\n",
      "Verification PASSED: Each expert has a completely unique set of 450 images!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "EXPERT_DISTRIBUTION_DIR = Path(\"C:/Users/SouayedBelkiss/OneDrive - gae/Desktop/Thesis/phase3_label_studio/Expert_distribution\")\n",
    "\n",
    "# Expert names\n",
    "EXPERTS = [\n",
    "    \"Martin Kappus\",\n",
    "    \"Luisa Carrer\",\n",
    "    \"Katrin Andermatt\",\n",
    "    \"Alexa Lintner\"\n",
    "]\n",
    "\n",
    "print(\"Verifying image distribution across experts...\")\n",
    "\n",
    "# Dictionary to track all unique images seen\n",
    "all_unique_images = set()\n",
    "has_overlap = False\n",
    "\n",
    "# Verify each expert's folders\n",
    "for expert in EXPERTS:\n",
    "    expert_dir = EXPERT_DISTRIBUTION_DIR / expert\n",
    "    iaa_dir = expert_dir / \"IAA_set\"\n",
    "    unique_dir = expert_dir / \"unique_set\"\n",
    "    \n",
    "    # Count IAA images\n",
    "    iaa_images = [f for f in os.listdir(iaa_dir) if f.lower().endswith('.png')]\n",
    "    iaa_count = len(iaa_images)\n",
    "    \n",
    "    # Count unique images\n",
    "    unique_images = [f for f in os.listdir(unique_dir) if f.lower().endswith('.png')]\n",
    "    unique_count = len(unique_images)\n",
    "    \n",
    "    # Check for overlaps in unique images\n",
    "    unique_image_set = set(unique_images)\n",
    "    overlaps = unique_image_set.intersection(all_unique_images)\n",
    "    \n",
    "    if overlaps:\n",
    "        has_overlap = True\n",
    "        print(f\"WARNING: {expert} shares {len(overlaps)} unique images with other experts\")\n",
    "    \n",
    "    # Add current expert's unique images to the set of all seen images\n",
    "    all_unique_images.update(unique_image_set)\n",
    "    \n",
    "    # Report counts\n",
    "    print(f\"{expert}:\")\n",
    "    print(f\"  IAA images: {iaa_count}/200 {'✓' if iaa_count == 200 else '✗'}\")\n",
    "    print(f\"  Unique images: {unique_count}/450 {'✓' if unique_count == 450 else '✗'}\")\n",
    "\n",
    "# Final verification\n",
    "if not has_overlap:\n",
    "    print(\"\\nVerification PASSED: Each expert has a completely unique set of 450 images!\")\n",
    "else:\n",
    "    print(\"\\nVerification FAILED: Some experts share unique images!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
