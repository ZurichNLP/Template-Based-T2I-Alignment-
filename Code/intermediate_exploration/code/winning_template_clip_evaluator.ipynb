{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Processing sample: asset_164\n",
      "Text truncated from 88 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.3585\n",
      "Processing sample: simpa_289\n",
      "Text truncated from 81 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.3268\n",
      "Processing sample: onestop_306\n",
      "  Basic Object Focus (Refined): 0.4363\n",
      "Processing sample: asset_494\n",
      "Text truncated from 94 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.2740\n",
      "Processing sample: onestop_064\n",
      "Text truncated from 79 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.3674\n",
      "Processing sample: simpa_307\n",
      "Text truncated from 84 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.2926\n",
      "Processing sample: simpa_137\n",
      "Text truncated from 72 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.2737\n",
      "Processing sample: asset_484\n",
      "Text truncated from 87 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.3578\n",
      "Processing sample: wikipedia_292\n",
      "Text truncated from 132 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.4295\n",
      "Processing sample: onestop_166\n",
      "Text truncated from 157 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.3960\n",
      "Processing sample: simpa_010\n",
      "  Image not found for Basic Object Focus (Refined)\n",
      "    Tried: ..\\images\\winning_template_samples\\by_template\\basic_object_focus_(refined)_(refined)\\simpa_010.png\n",
      "Processing sample: wikipedia_334\n",
      "Text truncated from 88 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.3529\n",
      "Processing sample: onestop_432\n",
      "Text truncated from 115 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.3773\n",
      "Processing sample: onestop_008\n",
      "Text truncated from 74 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.3160\n",
      "Processing sample: simpa_068\n",
      "Text truncated from 90 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.3513\n",
      "Processing sample: wikipedia_328\n",
      "Text truncated from 91 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.3124\n",
      "Processing sample: wikipedia_114\n",
      "Text truncated from 72 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.3454\n",
      "Processing sample: asset_345\n",
      "Text truncated from 101 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.3320\n",
      "Processing sample: wikipedia_365\n",
      "Text truncated from 79 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.3436\n",
      "Processing sample: asset_284\n",
      "Text truncated from 96 words to 60 words\n",
      "  Basic Object Focus (Refined): 0.3408\n",
      "\n",
      "Processed 19 image-prompt pairs\n",
      "Average CLIP score: 0.3465\n",
      "Score range: 0.2737 - 0.4363\n",
      "\n",
      "Scores by template:\n",
      "                                mean     std  count\n",
      "template_name                                      \n",
      "Basic Object Focus (Refined)  0.3465  0.0443     19\n",
      "\n",
      "Detailed results saved to 'clip_scores.csv'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import clip\n",
    "\n",
    "# Fix for newer torchvision versions\n",
    "try:\n",
    "    from torchvision.transforms.functional import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    try:\n",
    "        from torchvision.transforms.functional import InterpolateMode\n",
    "        BICUBIC = InterpolateMode.BICUBIC\n",
    "    except ImportError:\n",
    "        from PIL import Image\n",
    "        BICUBIC = Image.BICUBIC\n",
    "\n",
    "class SimpleCLIPScorer:\n",
    "    def __init__(self):\n",
    "        # Load CLIP model\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
    "        \n",
    "        # Image preprocessing\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "                               (0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "\n",
    "    def truncate_text(self, text: str) -> str:\n",
    "        \"\"\"Truncate text to fit CLIP's context length by word count.\"\"\"\n",
    "        # Simple approach: truncate to approximately 60 words to stay under token limit\n",
    "        words = text.split()\n",
    "        if len(words) > 60:\n",
    "            truncated = \" \".join(words[:60])\n",
    "            print(f\"Text truncated from {len(words)} words to 60 words\")\n",
    "            return truncated\n",
    "        return text\n",
    "\n",
    "    def load_and_preprocess_image(self, image_path: str) -> torch.Tensor:\n",
    "        \"\"\"Load and preprocess a single image.\"\"\"\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            return self.transform(image).unsqueeze(0).to(self.device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def calculate_clip_score(self, image_tensor: torch.Tensor, text: str) -> float:\n",
    "        \"\"\"Calculate CLIP score between image and text.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Truncate text if too long\n",
    "            truncated_text = self.truncate_text(text)\n",
    "            \n",
    "            # Encode text with truncation enabled\n",
    "            text_tokens = clip.tokenize([truncated_text], truncate=True).to(self.device)\n",
    "            text_features = self.model.encode_text(text_tokens)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # Encode image\n",
    "            image_features = self.model.encode_image(image_tensor)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # Calculate similarity\n",
    "            similarity = (100.0 * image_features @ text_features.T).item()\n",
    "            return similarity / 100.0  # Normalize to 0-1 range\n",
    "\n",
    "    def score_all_images(self, json_path: str, images_base_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Score all images against their prompts.\"\"\"\n",
    "        # Load prompts\n",
    "        with open(json_path, 'r') as f:\n",
    "            samples = json.load(f)\n",
    "\n",
    "        results = []\n",
    "        \n",
    "        # Process each sample\n",
    "        for sample in samples:\n",
    "            sample_id = sample['id']\n",
    "            print(f\"Processing sample: {sample_id}\")\n",
    "            \n",
    "            for template in sample['template_prompts']:\n",
    "                template_name = template['template_name']\n",
    "                prompt = template['prompt']\n",
    "                \n",
    "                # Try different possible image paths\n",
    "                possible_paths = [\n",
    "                    # Your structure: by_template/template_name_(refined)/sample_id.png\n",
    "                    os.path.join(images_base_path, 'by_template', \n",
    "                                f\"{template_name.lower().replace(' ', '_')}_(refined)\", \n",
    "                                f\"{sample_id}.png\"),\n",
    "                    \n",
    "                    # Alternative: by_template/template_name/sample_id.png\n",
    "                    os.path.join(images_base_path, 'by_template', \n",
    "                                template_name.lower().replace(' ', '_'), \n",
    "                                f\"{sample_id}.png\"),\n",
    "                    \n",
    "                    # Alternative: by_sample/sample_id/template_name.png\n",
    "                    os.path.join(images_base_path, 'by_sample', sample_id,\n",
    "                                f\"{template_name.lower().replace(' ', '_')}.png\"),\n",
    "                ]\n",
    "                \n",
    "                image_path = None\n",
    "                for path in possible_paths:\n",
    "                    if os.path.exists(path):\n",
    "                        image_path = path\n",
    "                        break\n",
    "                \n",
    "                if image_path:\n",
    "                    # Calculate CLIP score\n",
    "                    image_tensor = self.load_and_preprocess_image(image_path)\n",
    "                    if image_tensor is not None:\n",
    "                        score = self.calculate_clip_score(image_tensor, prompt)\n",
    "                        \n",
    "                        results.append({\n",
    "                            'sample_id': sample_id,\n",
    "                            'template_name': template_name,\n",
    "                            'clip_score': score,\n",
    "                            'image_path': image_path,\n",
    "                            'prompt_length': len(prompt)\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"  {template_name}: {score:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"  Failed to load image: {image_path}\")\n",
    "                else:\n",
    "                    print(f\"  Image not found for {template_name}\")\n",
    "                    print(f\"    Tried: {possible_paths[0]}\")\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "def main():\n",
    "    # Initialize scorer\n",
    "    scorer = SimpleCLIPScorer()\n",
    "    \n",
    "    # Paths - adjust these to match your setup\n",
    "    json_path = os.path.join('..', 'output_files', 'winning_template_prompts.json')\n",
    "    images_path = os.path.join('..', 'images', 'winning_template_samples')\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"JSON file not found: {json_path}\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(images_path):\n",
    "        print(f\"Images directory not found: {images_path}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate scores\n",
    "    results_df = scorer.score_all_images(json_path, images_path)\n",
    "    \n",
    "    if results_df.empty:\n",
    "        print(\"No results found. Check your file paths.\")\n",
    "        return\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nProcessed {len(results_df)} image-prompt pairs\")\n",
    "    print(f\"Average CLIP score: {results_df['clip_score'].mean():.4f}\")\n",
    "    print(f\"Score range: {results_df['clip_score'].min():.4f} - {results_df['clip_score'].max():.4f}\")\n",
    "    \n",
    "    # Show results by template\n",
    "    print(\"\\nScores by template:\")\n",
    "    template_stats = results_df.groupby('template_name')['clip_score'].agg(['mean', 'std', 'count'])\n",
    "    print(template_stats.round(4))\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv('clip_scores.csv', index=False)\n",
    "    print(\"\\nDetailed results saved to 'clip_scores.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
