{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b77d5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ANALYZING CONCERN AND YES/NO FIELD STRUCTURES\n",
      "================================================================================\n",
      "\n",
      "üìÅ Loading Expert_A annotations...\n",
      "   Loaded 250 records\n",
      "   concerns: 240 entries found\n",
      "   no_text: 250 entries found\n",
      "   well_separated_obj: 250 entries found\n",
      "\n",
      "üìÅ Loading Expert_K annotations...\n",
      "   Loaded 250 records\n",
      "   concerns: 241 entries found\n",
      "   no_text: 250 entries found\n",
      "   well_separated_obj: 250 entries found\n",
      "\n",
      "üìÅ Loading Expert_L annotations...\n",
      "   Loaded 200 records\n",
      "   concerns: 139 entries found\n",
      "   no_text: 200 entries found\n",
      "   well_separated_obj: 200 entries found\n",
      "\n",
      "üìÅ Loading Expert_M annotations...\n",
      "   Loaded 276 records\n",
      "   concerns: 49 entries found\n",
      "   no_text: 276 entries found\n",
      "   well_separated_obj: 276 entries found\n",
      "\n",
      "============================================================\n",
      "DETAILED ANALYSIS: CONCERNS\n",
      "============================================================\n",
      "\n",
      "--- Expert_A ---\n",
      "Total entries: 240\n",
      "Value types: {\"<class 'dict'>\": 10}\n",
      "Sample values:\n",
      "  1: {'choices': ['Too complex for target audience', 'Misleading representation', 'Poor accessibility']}\n",
      "  2: {'choices': ['Too complex for target audience', 'Misleading representation', 'Poor accessibility']}\n",
      "  3: {'choices': ['Discriminatory content', 'Too complex for target audience', 'Misleading representation', 'Poor accessibility']}\n",
      "  4: {'choices': ['Too complex for target audience', 'Misleading representation', 'Poor accessibility']}\n",
      "  5: {'choices': ['Too complex for target audience', 'Misleading representation', 'Poor accessibility']}\n",
      "Empty concern responses: 0\n",
      "Total concern selections: 564\n",
      "Top concerns:\n",
      "  'Misleading representation': 220\n",
      "  'Poor accessibility': 193\n",
      "  'Too complex for target audience': 144\n",
      "  'Discriminatory content': 6\n",
      "  'Culturally insensitive': 1\n",
      "\n",
      "--- Expert_K ---\n",
      "Total entries: 241\n",
      "Value types: {\"<class 'dict'>\": 10}\n",
      "Sample values:\n",
      "  1: {'choices': ['Too complex for target audience', 'Misleading representation', 'Poor accessibility']}\n",
      "  2: {'choices': ['Too complex for target audience', 'Misleading representation', 'Poor accessibility']}\n",
      "  3: {'choices': ['Misleading representation', 'Poor accessibility']}\n",
      "  4: {'choices': ['Too complex for target audience', 'Misleading representation', 'Poor accessibility']}\n",
      "  5: {'choices': ['Too complex for target audience', 'Misleading representation', 'Poor accessibility']}\n",
      "Empty concern responses: 0\n",
      "Total concern selections: 450\n",
      "Top concerns:\n",
      "  'Misleading representation': 234\n",
      "  'Poor accessibility': 125\n",
      "  'Too complex for target audience': 79\n",
      "  'Potentially triggering': 9\n",
      "  'Discriminatory content': 2\n",
      "  'Culturally insensitive': 1\n",
      "\n",
      "--- Expert_L ---\n",
      "Total entries: 139\n",
      "Value types: {\"<class 'dict'>\": 10}\n",
      "Sample values:\n",
      "  1: {'choices': ['Too complex for target audience', 'Misleading representation', 'Poor accessibility']}\n",
      "  2: {'choices': ['Too complex for target audience', 'Misleading representation']}\n",
      "  3: {'choices': ['Too complex for target audience', 'Misleading representation', 'Poor accessibility']}\n",
      "  4: {'choices': ['Too complex for target audience', 'Misleading representation']}\n",
      "  5: {'choices': ['Discriminatory content', 'Too complex for target audience', 'Misleading representation']}\n",
      "Empty concern responses: 0\n",
      "Total concern selections: 216\n",
      "Top concerns:\n",
      "  'Misleading representation': 110\n",
      "  'Too complex for target audience': 62\n",
      "  'Poor accessibility': 22\n",
      "  'Discriminatory content': 14\n",
      "  'Culturally insensitive': 4\n",
      "  'Potentially triggering': 4\n",
      "\n",
      "--- Expert_M ---\n",
      "Total entries: 49\n",
      "Value types: {\"<class 'dict'>\": 10}\n",
      "Sample values:\n",
      "  1: {'choices': ['Discriminatory content']}\n",
      "  2: {'choices': ['Discriminatory content']}\n",
      "  3: {'choices': ['Misleading representation']}\n",
      "  4: {'choices': ['Too complex for target audience', 'Misleading representation']}\n",
      "  5: {'choices': ['Misleading representation', 'Poor accessibility']}\n",
      "Empty concern responses: 0\n",
      "Total concern selections: 53\n",
      "Top concerns:\n",
      "  'Discriminatory content': 15\n",
      "  'Too complex for target audience': 14\n",
      "  'Misleading representation': 10\n",
      "  'Potentially triggering': 6\n",
      "  'Culturally insensitive': 5\n",
      "  'Poor accessibility': 3\n",
      "\n",
      "============================================================\n",
      "DETAILED ANALYSIS: NO_TEXT\n",
      "============================================================\n",
      "\n",
      "--- Expert_A ---\n",
      "Total entries: 250\n",
      "Value types: {\"<class 'dict'>\": 10}\n",
      "Sample values:\n",
      "  1: {'choices': ['Yes']}\n",
      "  2: {'choices': ['Yes']}\n",
      "  3: {'choices': ['No']}\n",
      "  4: {'choices': ['Yes']}\n",
      "  5: {'choices': ['Yes']}\n",
      "Empty responses: 0\n",
      "Response distribution:\n",
      "  'Yes': 150 (60.0%)\n",
      "  'No': 100 (40.0%)\n",
      "\n",
      "--- Expert_K ---\n",
      "Total entries: 250\n",
      "Value types: {\"<class 'dict'>\": 10}\n",
      "Sample values:\n",
      "  1: {'choices': ['Yes']}\n",
      "  2: {'choices': ['No']}\n",
      "  3: {'choices': ['Yes']}\n",
      "  4: {'choices': ['Yes']}\n",
      "  5: {'choices': ['Yes']}\n",
      "Empty responses: 0\n",
      "Response distribution:\n",
      "  'Yes': 86 (34.4%)\n",
      "  'No': 164 (65.6%)\n",
      "\n",
      "--- Expert_L ---\n",
      "Total entries: 200\n",
      "Value types: {\"<class 'dict'>\": 10}\n",
      "Sample values:\n",
      "  1: {'choices': ['Yes']}\n",
      "  2: {'choices': ['Yes']}\n",
      "  3: {'choices': ['Yes']}\n",
      "  4: {'choices': ['Yes']}\n",
      "  5: {'choices': ['Yes']}\n",
      "Empty responses: 0\n",
      "Response distribution:\n",
      "  'Yes': 118 (59.0%)\n",
      "  'No': 82 (41.0%)\n",
      "\n",
      "--- Expert_M ---\n",
      "Total entries: 276\n",
      "Value types: {\"<class 'dict'>\": 10}\n",
      "Sample values:\n",
      "  1: {'choices': ['No']}\n",
      "  2: {'choices': ['Yes']}\n",
      "  3: {'choices': ['No']}\n",
      "  4: {'choices': ['Yes']}\n",
      "  5: {'choices': ['No']}\n",
      "Empty responses: 0\n",
      "Response distribution:\n",
      "  'No': 168 (60.9%)\n",
      "  'Yes': 108 (39.1%)\n",
      "\n",
      "============================================================\n",
      "DETAILED ANALYSIS: WELL_SEPARATED_OBJ\n",
      "============================================================\n",
      "\n",
      "--- Expert_A ---\n",
      "Total entries: 250\n",
      "Value types: {\"<class 'dict'>\": 10}\n",
      "Sample values:\n",
      "  1: {'choices': ['Yes']}\n",
      "  2: {'choices': ['Yes']}\n",
      "  3: {'choices': ['Yes']}\n",
      "  4: {'choices': ['Yes']}\n",
      "  5: {'choices': ['No']}\n",
      "Empty responses: 0\n",
      "Response distribution:\n",
      "  'Yes': 146 (58.4%)\n",
      "  'No': 104 (41.6%)\n",
      "\n",
      "--- Expert_K ---\n",
      "Total entries: 250\n",
      "Value types: {\"<class 'dict'>\": 10}\n",
      "Sample values:\n",
      "  1: {'choices': ['No']}\n",
      "  2: {'choices': ['Yes']}\n",
      "  3: {'choices': ['Yes']}\n",
      "  4: {'choices': ['Yes']}\n",
      "  5: {'choices': ['Yes']}\n",
      "Empty responses: 0\n",
      "Response distribution:\n",
      "  'No': 36 (14.4%)\n",
      "  'Yes': 214 (85.6%)\n",
      "\n",
      "--- Expert_L ---\n",
      "Total entries: 200\n",
      "Value types: {\"<class 'dict'>\": 10}\n",
      "Sample values:\n",
      "  1: {'choices': ['No']}\n",
      "  2: {'choices': ['Yes']}\n",
      "  3: {'choices': ['Yes']}\n",
      "  4: {'choices': ['Yes']}\n",
      "  5: {'choices': ['Yes']}\n",
      "Empty responses: 0\n",
      "Response distribution:\n",
      "  'No': 73 (36.5%)\n",
      "  'Yes': 127 (63.5%)\n",
      "\n",
      "--- Expert_M ---\n",
      "Total entries: 276\n",
      "Value types: {\"<class 'dict'>\": 10}\n",
      "Sample values:\n",
      "  1: {'choices': ['No']}\n",
      "  2: {'choices': ['Yes']}\n",
      "  3: {'choices': ['Yes']}\n",
      "  4: {'choices': ['Yes']}\n",
      "  5: {'choices': ['Yes']}\n",
      "Empty responses: 0\n",
      "Response distribution:\n",
      "  'No': 20 (7.2%)\n",
      "  'Yes': 256 (92.8%)\n",
      "\n",
      "============================================================\n",
      "CROSS-EXPERT COMPARISON\n",
      "============================================================\n",
      "\n",
      "CONCERNS - Coverage by Expert:\n",
      "  Expert_A: 240/250 (96.0%)\n",
      "  Expert_K: 241/250 (96.4%)\n",
      "  Expert_L: 139/200 (69.5%)\n",
      "  Expert_M: 49/276 (17.8%)\n",
      "\n",
      "Summary statistics for concerns:\n",
      "  Average coverage: 69.9%\n",
      "  Coverage range: 17.8% - 96.4%\n",
      "\n",
      "NO_TEXT - Coverage by Expert:\n",
      "  Expert_A: 250/250 (100.0%)\n",
      "  Expert_K: 250/250 (100.0%)\n",
      "  Expert_L: 200/200 (100.0%)\n",
      "  Expert_M: 276/276 (100.0%)\n",
      "\n",
      "Summary statistics for no_text:\n",
      "  Average coverage: 100.0%\n",
      "  Coverage range: 100.0% - 100.0%\n",
      "\n",
      "WELL_SEPARATED_OBJ - Coverage by Expert:\n",
      "  Expert_A: 250/250 (100.0%)\n",
      "  Expert_K: 250/250 (100.0%)\n",
      "  Expert_L: 200/200 (100.0%)\n",
      "  Expert_M: 276/276 (100.0%)\n",
      "\n",
      "Summary statistics for well_separated_obj:\n",
      "  Average coverage: 100.0%\n",
      "  Coverage range: 100.0% - 100.0%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Configuration\n",
    "ANNOTATION_DIR = r\"C:\\Users\\SouayedBelkiss\\OneDrive - gae\\Desktop\\Thesis\\annotation_analysis\\expert work\"\n",
    "ANNOTATION_FILES = {\n",
    "    'Expert_A': 'alexa_annotations.json',\n",
    "    'Expert_K': 'katrin_annotations.json', \n",
    "    'Expert_L': 'luisa_annotations.json',\n",
    "    'Expert_M': 'martin_annotations.json'\n",
    "}\n",
    "\n",
    "def load_annotations(expert_name, filename):\n",
    "    \"\"\"Load annotations from JSON file\"\"\"\n",
    "    filepath = os.path.join(ANNOTATION_DIR, filename)\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {expert_name} annotations: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_field_data(annotations, field_name):\n",
    "    \"\"\"Extract all values for a specific field across all annotations\"\"\"\n",
    "    field_values = []\n",
    "    \n",
    "    for record in annotations:\n",
    "        if 'annotations' in record and len(record['annotations']) > 0:\n",
    "            annotation = record['annotations'][0]\n",
    "            if 'result' in annotation:\n",
    "                for item in annotation['result']:\n",
    "                    if item.get('from_name') == field_name:\n",
    "                        field_values.append({\n",
    "                            'image': record['data'].get('original_filename', 'unknown'),\n",
    "                            'value': item.get('value', {}),\n",
    "                            'type': item.get('type', 'unknown')\n",
    "                        })\n",
    "    \n",
    "    return field_values\n",
    "\n",
    "def analyze_field_structure():\n",
    "    \"\"\"Analyze the structure of concern and yes/no fields across all experts\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ANALYZING CONCERN AND YES/NO FIELD STRUCTURES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Fields to analyze\n",
    "    target_fields = ['concerns', 'no_text', 'well_separated_obj']\n",
    "    \n",
    "    all_experts_data = {}\n",
    "    \n",
    "    # Load data for each expert\n",
    "    for expert_name, filename in ANNOTATION_FILES.items():\n",
    "        print(f\"\\nüìÅ Loading {expert_name} annotations...\")\n",
    "        annotations = load_annotations(expert_name, filename)\n",
    "        print(f\"   Loaded {len(annotations)} records\")\n",
    "        \n",
    "        all_experts_data[expert_name] = {}\n",
    "        \n",
    "        # Extract data for each target field\n",
    "        for field in target_fields:\n",
    "            field_data = extract_field_data(annotations, field)\n",
    "            all_experts_data[expert_name][field] = field_data\n",
    "            print(f\"   {field}: {len(field_data)} entries found\")\n",
    "    \n",
    "    # Detailed analysis for each field\n",
    "    for field in target_fields:\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(f\"DETAILED ANALYSIS: {field.upper()}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for expert_name in ANNOTATION_FILES.keys():\n",
    "            field_data = all_experts_data[expert_name][field]\n",
    "            print(f\"\\n--- {expert_name} ---\")\n",
    "            print(f\"Total entries: {len(field_data)}\")\n",
    "            \n",
    "            if len(field_data) > 0:\n",
    "                # Analyze value structure\n",
    "                value_types = Counter()\n",
    "                sample_values = []\n",
    "                \n",
    "                for entry in field_data[:10]:  # First 10 samples\n",
    "                    value = entry['value']\n",
    "                    value_types[str(type(value))] += 1\n",
    "                    sample_values.append(value)\n",
    "                \n",
    "                print(f\"Value types: {dict(value_types)}\")\n",
    "                print(\"Sample values:\")\n",
    "                for i, val in enumerate(sample_values[:5]):\n",
    "                    print(f\"  {i+1}: {val}\")\n",
    "                \n",
    "                # For concerns field, analyze choice patterns\n",
    "                if field == 'concerns':\n",
    "                    all_concerns = []\n",
    "                    empty_count = 0\n",
    "                    \n",
    "                    for entry in field_data:\n",
    "                        choices = entry['value'].get('choices', [])\n",
    "                        if choices:\n",
    "                            all_concerns.extend(choices)\n",
    "                        else:\n",
    "                            empty_count += 1\n",
    "                    \n",
    "                    print(f\"Empty concern responses: {empty_count}\")\n",
    "                    print(f\"Total concern selections: {len(all_concerns)}\")\n",
    "                    \n",
    "                    if all_concerns:\n",
    "                        concern_counts = Counter(all_concerns)\n",
    "                        print(\"Top concerns:\")\n",
    "                        for concern, count in concern_counts.most_common(10):\n",
    "                            print(f\"  '{concern}': {count}\")\n",
    "                \n",
    "                # For yes/no fields, analyze response patterns\n",
    "                elif field in ['no_text', 'well_separated_obj']:\n",
    "                    response_counts = Counter()\n",
    "                    empty_count = 0\n",
    "                    \n",
    "                    for entry in field_data:\n",
    "                        choices = entry['value'].get('choices', [])\n",
    "                        if choices:\n",
    "                            # Assume single choice for yes/no questions\n",
    "                            response_counts[choices[0]] += 1\n",
    "                        else:\n",
    "                            empty_count += 1\n",
    "                    \n",
    "                    print(f\"Empty responses: {empty_count}\")\n",
    "                    print(\"Response distribution:\")\n",
    "                    for response, count in response_counts.items():\n",
    "                        percentage = (count / len(field_data)) * 100\n",
    "                        print(f\"  '{response}': {count} ({percentage:.1f}%)\")\n",
    "            \n",
    "            else:\n",
    "                print(\"No data found for this field\")\n",
    "    \n",
    "    # Cross-expert comparison\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"CROSS-EXPERT COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for field in target_fields:\n",
    "        print(f\"\\n{field.upper()} - Coverage by Expert:\")\n",
    "        coverage_data = []\n",
    "        \n",
    "        for expert_name in ANNOTATION_FILES.keys():\n",
    "            field_data = all_experts_data[expert_name][field]\n",
    "            total_annotations = len(load_annotations(expert_name, ANNOTATION_FILES[expert_name]))\n",
    "            coverage = len(field_data) / total_annotations * 100 if total_annotations > 0 else 0\n",
    "            coverage_data.append({\n",
    "                'Expert': expert_name,\n",
    "                'Field_Entries': len(field_data),\n",
    "                'Total_Annotations': total_annotations,\n",
    "                'Coverage_Percent': coverage\n",
    "            })\n",
    "            print(f\"  {expert_name}: {len(field_data)}/{total_annotations} ({coverage:.1f}%)\")\n",
    "        \n",
    "        # Calculate summary statistics manually\n",
    "        coverages = [item['Coverage_Percent'] for item in coverage_data]\n",
    "        avg_coverage = sum(coverages) / len(coverages) if coverages else 0\n",
    "        min_coverage = min(coverages) if coverages else 0\n",
    "        max_coverage = max(coverages) if coverages else 0\n",
    "        \n",
    "        print(f\"\\nSummary statistics for {field}:\")\n",
    "        print(f\"  Average coverage: {avg_coverage:.1f}%\")\n",
    "        print(f\"  Coverage range: {min_coverage:.1f}% - {max_coverage:.1f}%\")\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_field_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bf4102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Concern Pattern and Yes/No Analysis\n",
      "================================================================================\n",
      "Loading Expert_A data...\n",
      "  Extracted data for 250 images\n",
      "Loading Expert_K data...\n",
      "  Extracted data for 250 images\n",
      "Loading Expert_L data...\n",
      "  Extracted data for 200 images\n",
      "Loading Expert_M data...\n",
      "  Extracted data for 276 images\n",
      "\n",
      "================================================================================\n",
      "üö® CONCERN PATTERN ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä MOST COMMON CONCERNS (Overall)\n",
      "--------------------------------------------------\n",
      "Misleading representation          : 574 ( 44.7%)\n",
      "Poor accessibility                 : 343 ( 26.7%)\n",
      "Too complex for target audience    : 299 ( 23.3%)\n",
      "Discriminatory content             :  37 (  2.9%)\n",
      "Potentially triggering             :  19 (  1.5%)\n",
      "Culturally insensitive             :  11 (  0.9%)\n",
      "\n",
      "üèÜ MOST PICKED CONCERN BY EACH EXPERT\n",
      "--------------------------------------------------\n",
      "Expert_A: 'Misleading representation' (220/564 = 39.0%)\n",
      "Expert_K: 'Misleading representation' (234/450 = 52.0%)\n",
      "Expert_L: 'Misleading representation' (110/216 = 50.9%)\n",
      "Expert_M: 'Discriminatory content' (15/53 = 28.3%)\n",
      "\n",
      "üéØ EXPERT CONCERN SPECIALTIES\n",
      "--------------------------------------------------\n",
      "\n",
      "Expert_A concern breakdown:\n",
      "  Misleading representation          : 220 ( 39.0%)\n",
      "  Poor accessibility                 : 193 ( 34.2%)\n",
      "  Too complex for target audience    : 144 ( 25.5%)\n",
      "  Discriminatory content             :   6 (  1.1%)\n",
      "  Culturally insensitive             :   1 (  0.2%)\n",
      "\n",
      "Expert_K concern breakdown:\n",
      "  Misleading representation          : 234 ( 52.0%)\n",
      "  Poor accessibility                 : 125 ( 27.8%)\n",
      "  Too complex for target audience    :  79 ( 17.6%)\n",
      "  Potentially triggering             :   9 (  2.0%)\n",
      "  Discriminatory content             :   2 (  0.4%)\n",
      "  Culturally insensitive             :   1 (  0.2%)\n",
      "\n",
      "Expert_L concern breakdown:\n",
      "  Misleading representation          : 110 ( 50.9%)\n",
      "  Too complex for target audience    :  62 ( 28.7%)\n",
      "  Poor accessibility                 :  22 ( 10.2%)\n",
      "  Discriminatory content             :  14 (  6.5%)\n",
      "  Culturally insensitive             :   4 (  1.9%)\n",
      "  Potentially triggering             :   4 (  1.9%)\n",
      "\n",
      "Expert_M concern breakdown:\n",
      "  Discriminatory content             :  15 ( 28.3%)\n",
      "  Too complex for target audience    :  14 ( 26.4%)\n",
      "  Misleading representation          :  10 ( 18.9%)\n",
      "  Potentially triggering             :   6 ( 11.3%)\n",
      "  Culturally insensitive             :   5 (  9.4%)\n",
      "  Poor accessibility                 :   3 (  5.7%)\n",
      "\n",
      "üìã CONCERN FREQUENCY BY EXPERT\n",
      "--------------------------------------------------\n",
      "Concern                                  A      K      L      M   Total\n",
      "------------------------------------------------------------------------\n",
      "Too complex for target audience        144     79     62     14     299\n",
      "Misleading representation              220    234    110     10     574\n",
      "Poor accessibility                     193    125     22      3     343\n",
      "Discriminatory content                   6      2     14     15      37\n",
      "Culturally insensitive                   1      1      4      5      11\n",
      "Potentially triggering                   0      9      4      6      19\n",
      "\n",
      "üîó CONCERN CO-OCCURRENCE ANALYSIS\n",
      "--------------------------------------------------\n",
      "Images with multiple concerns: 400\n",
      "\n",
      "Top concern combinations:\n",
      "  Misleading representation + Poor accessibility: 313\n",
      "  Misleading representation + Too complex for target audience: 248\n",
      "  Poor accessibility + Too complex for target audience: 202\n",
      "  Discriminatory content + Misleading representation: 21\n",
      "  Misleading representation + Potentially triggering: 13\n",
      "  Discriminatory content + Poor accessibility: 9\n",
      "  Discriminatory content + Too complex for target audience: 9\n",
      "  Culturally insensitive + Misleading representation: 5\n",
      "  Culturally insensitive + Discriminatory content: 4\n",
      "  Poor accessibility + Potentially triggering: 4\n",
      "\n",
      "ü§ù EXPERT AGREEMENT ON CONCERNS\n",
      "--------------------------------------------------\n",
      "Images annotated by multiple experts: 113\n",
      "Average concern agreement (Jaccard): 0.259\n",
      "Agreement range: 0.000 - 1.000\n",
      "‚ö†Ô∏è  No mapping file found - skipping style-specific analysis\n",
      "\n",
      "================================================================================\n",
      "‚úÖ YES/NO QUESTION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä NO TEXT ANALYSIS\n",
      "--------------------------------------------------\n",
      "Overall distribution:\n",
      "  Yes: 462 (47.3%)\n",
      "  No: 514 (52.7%)\n",
      "\n",
      "Distribution by expert:\n",
      "Expert        Yes     No    Yes %\n",
      "--------------------------------\n",
      "Expert_A      150    100    60.0%\n",
      "Expert_K       86    164    34.4%\n",
      "Expert_L      118     82    59.0%\n",
      "Expert_M      108    168    39.1%\n",
      "\n",
      "ü§ù Expert Agreement for No Text\n",
      "----------------------------------------\n",
      "Images with multiple annotations: 113\n",
      "Perfect agreement: 86/113 (76.1%)\n",
      "\n",
      "üìä WELL SEPARATED OBJ ANALYSIS\n",
      "--------------------------------------------------\n",
      "Overall distribution:\n",
      "  Yes: 743 (76.1%)\n",
      "  No: 233 (23.9%)\n",
      "\n",
      "Distribution by expert:\n",
      "Expert        Yes     No    Yes %\n",
      "--------------------------------\n",
      "Expert_A      146    104    58.4%\n",
      "Expert_K      214     36    85.6%\n",
      "Expert_L      127     73    63.5%\n",
      "Expert_M      256     20    92.8%\n",
      "\n",
      "ü§ù Expert Agreement for Well Separated Obj\n",
      "----------------------------------------\n",
      "Images with multiple annotations: 113\n",
      "Perfect agreement: 78/113 (69.0%)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_visualizations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 379\u001b[39m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 373\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    370\u001b[39m analyze_yesno_patterns(all_data, mapping_df)\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# Create visualizations\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m \u001b[43mcreate_visualizations\u001b[49m(overall_concern_counts, expert_concern_counts, all_data)\n\u001b[32m    375\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéâ Analysis complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    376\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'create_visualizations' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main analysis function\"\"\"\n",
    "    print(\"üöÄ Starting Concern Pattern and Yes/No Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load all data\n",
    "    all_data = {}\n",
    "    for expert_name, filename in ANNOTATION_FILES.items():\n",
    "        print(f\"Loading {expert_name} data...\")\n",
    "        annotations = load_annotations(expert_name, filename)\n",
    "        all_data[expert_name] = extract_structured_data(annotations, expert_name)\n",
    "        print(f\"  Extracted data for {len(all_data[expert_name])} images\")\n",
    "    \n",
    "    # Run analyses\n",
    "    overall_concern_counts, expert_concern_counts, concern_pairs = analyze_concern_patterns(all_data)\n",
    "    \n",
    "    # Try to load mapping file for style analysis\n",
    "    mapping_df = None\n",
    "    try:\n",
    "        # You can uncomment and adjust this path when you want style analysis\n",
    "        # mapping_df = pd.read_csv('path_to_your_mapping_file.csv')\n",
    "        # print(\"‚úÖ Mapping file loaded for style analysis\")\n",
    "        pass\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  No mapping file found - skipping style-specific analysis\")\n",
    "    \n",
    "    analyze_yesno_patterns(all_data, mapping_df)\n",
    "    \n",
    "    print(\"\\nüéâ Analysis complete!\")\n",
    "    print(\"=\"*80)\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "# Configuration\n",
    "ANNOTATION_DIR = r\"C:\\Users\\SouayedBelkiss\\OneDrive - gae\\Desktop\\Thesis\\annotation_analysis\\expert work\"\n",
    "ANNOTATION_FILES = {\n",
    "    'Expert_A': 'alexa_annotations.json',\n",
    "    'Expert_K': 'katrin_annotations.json', \n",
    "    'Expert_L': 'luisa_annotations.json',\n",
    "    'Expert_M': 'martin_annotations.json'\n",
    "}\n",
    "\n",
    "# Define all possible concerns based on your data\n",
    "ALL_CONCERNS = [\n",
    "    'Too complex for target audience',\n",
    "    'Misleading representation', \n",
    "    'Poor accessibility',\n",
    "    'Discriminatory content',\n",
    "    'Culturally insensitive',\n",
    "    'Potentially triggering'\n",
    "]\n",
    "\n",
    "def load_annotations(expert_name, filename):\n",
    "    \"\"\"Load annotations from JSON file\"\"\"\n",
    "    filepath = os.path.join(ANNOTATION_DIR, filename)\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {expert_name} annotations: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_structured_data(annotations, expert_name):\n",
    "    \"\"\"Extract structured data for analysis\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for record in annotations:\n",
    "        if 'annotations' in record and len(record['annotations']) > 0:\n",
    "            annotation = record['annotations'][0]\n",
    "            if 'result' in annotation:\n",
    "                \n",
    "                # Initialize record data\n",
    "                record_data = {\n",
    "                    'expert': expert_name,\n",
    "                    'image': record['data'].get('original_filename', 'unknown'),\n",
    "                    'concerns': [],\n",
    "                    'no_text': None,\n",
    "                    'well_separated_obj': None\n",
    "                }\n",
    "                \n",
    "                # Extract data from result items\n",
    "                for item in annotation['result']:\n",
    "                    field_name = item.get('from_name')\n",
    "                    value = item.get('value', {})\n",
    "                    \n",
    "                    if field_name == 'concerns':\n",
    "                        record_data['concerns'] = value.get('choices', [])\n",
    "                    elif field_name == 'no_text':\n",
    "                        choices = value.get('choices', [])\n",
    "                        record_data['no_text'] = choices[0] if choices else None\n",
    "                    elif field_name == 'well_separated_obj':\n",
    "                        choices = value.get('choices', [])\n",
    "                        record_data['well_separated_obj'] = choices[0] if choices else None\n",
    "                \n",
    "                data.append(record_data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def analyze_concern_patterns(all_data):\n",
    "    \"\"\"Analyze concern patterns across experts\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üö® CONCERN PATTERN ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Most common concerns overall\n",
    "    print(\"\\nüìä MOST COMMON CONCERNS (Overall)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    all_concerns = []\n",
    "    expert_concern_counts = defaultdict(Counter)\n",
    "    \n",
    "    for expert_data in all_data.values():\n",
    "        for record in expert_data:\n",
    "            concerns = record['concerns']\n",
    "            all_concerns.extend(concerns)\n",
    "            for concern in concerns:\n",
    "                expert_concern_counts[record['expert']][concern] += 1\n",
    "    \n",
    "    overall_concern_counts = Counter(all_concerns)\n",
    "    \n",
    "    for concern, count in overall_concern_counts.most_common():\n",
    "        percentage = (count / len(all_concerns)) * 100 if all_concerns else 0\n",
    "        print(f\"{concern:35}: {count:3d} ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # 1.5. Most picked concern by each expert\n",
    "    print(f\"\\nüèÜ MOST PICKED CONCERN BY EACH EXPERT\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for expert_name in ['Expert_A', 'Expert_K', 'Expert_L', 'Expert_M']:\n",
    "        expert_counts = expert_concern_counts[expert_name]\n",
    "        if expert_counts:\n",
    "            top_concern, top_count = expert_counts.most_common(1)[0]\n",
    "            total_expert_concerns = sum(expert_counts.values())\n",
    "            percentage = (top_count / total_expert_concerns) * 100\n",
    "            print(f\"{expert_name}: '{top_concern}' ({top_count}/{total_expert_concerns} = {percentage:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"{expert_name}: No concerns reported\")\n",
    "    \n",
    "    # 1.6. Expert specialty/focus analysis\n",
    "    print(f\"\\nüéØ EXPERT CONCERN SPECIALTIES\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Calculate each expert's concern distribution\n",
    "    for expert_name in ['Expert_A', 'Expert_K', 'Expert_L', 'Expert_M']:\n",
    "        expert_counts = expert_concern_counts[expert_name]\n",
    "        total_expert = sum(expert_counts.values())\n",
    "        \n",
    "        if total_expert > 0:\n",
    "            print(f\"\\n{expert_name} concern breakdown:\")\n",
    "            for concern, count in expert_counts.most_common():\n",
    "                pct = (count / total_expert) * 100\n",
    "                print(f\"  {concern:<35}: {count:3d} ({pct:5.1f}%)\")\n",
    "        else:\n",
    "            print(f\"\\n{expert_name}: No concerns reported\")\n",
    "    \n",
    "    # 2. Concern patterns by expert\n",
    "    print(f\"\\nüìã CONCERN FREQUENCY BY EXPERT\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Concern':<35} {'A':>6} {'K':>6} {'L':>6} {'M':>6} {'Total':>7}\")\n",
    "    print(\"-\" * 72)\n",
    "    \n",
    "    for concern in ALL_CONCERNS:\n",
    "        counts = [expert_concern_counts[expert][concern] for expert in ['Expert_A', 'Expert_K', 'Expert_L', 'Expert_M']]\n",
    "        total = sum(counts)\n",
    "        print(f\"{concern:<35} {counts[0]:6d} {counts[1]:6d} {counts[2]:6d} {counts[3]:6d} {total:7d}\")\n",
    "    \n",
    "    # 3. Concern co-occurrence analysis\n",
    "    print(f\"\\nüîó CONCERN CO-OCCURRENCE ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Find images with multiple concerns\n",
    "    multi_concern_records = []\n",
    "    for expert_data in all_data.values():\n",
    "        for record in expert_data:\n",
    "            if len(record['concerns']) > 1:\n",
    "                multi_concern_records.append(record)\n",
    "    \n",
    "    print(f\"Images with multiple concerns: {len(multi_concern_records)}\")\n",
    "    \n",
    "    # Analyze pairs of co-occurring concerns\n",
    "    concern_pairs = Counter()\n",
    "    for record in multi_concern_records:\n",
    "        concerns = record['concerns']\n",
    "        for pair in combinations(sorted(concerns), 2):\n",
    "            concern_pairs[pair] += 1\n",
    "    \n",
    "    print(f\"\\nTop concern combinations:\")\n",
    "    for pair, count in concern_pairs.most_common(10):\n",
    "        print(f\"  {pair[0]} + {pair[1]}: {count}\")\n",
    "    \n",
    "    # 4. Expert agreement on concerns (for images annotated by multiple experts)\n",
    "    print(f\"\\nü§ù EXPERT AGREEMENT ON CONCERNS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Group by image\n",
    "    image_concerns = defaultdict(list)\n",
    "    for expert_data in all_data.values():\n",
    "        for record in expert_data:\n",
    "            image_concerns[record['image']].append({\n",
    "                'expert': record['expert'],\n",
    "                'concerns': set(record['concerns'])\n",
    "            })\n",
    "    \n",
    "    # Only analyze images annotated by multiple experts\n",
    "    multi_expert_images = {img: data for img, data in image_concerns.items() if len(data) > 1}\n",
    "    \n",
    "    print(f\"Images annotated by multiple experts: {len(multi_expert_images)}\")\n",
    "    \n",
    "    if multi_expert_images:\n",
    "        agreement_scores = []\n",
    "        for img, expert_data in multi_expert_images.items():\n",
    "            if len(expert_data) >= 2:\n",
    "                # Calculate pairwise agreement\n",
    "                pairs = list(combinations(expert_data, 2))\n",
    "                for pair in pairs:\n",
    "                    concerns1 = pair[0]['concerns']\n",
    "                    concerns2 = pair[1]['concerns']\n",
    "                    \n",
    "                    if len(concerns1) == 0 and len(concerns2) == 0:\n",
    "                        agreement = 1.0  # Both have no concerns\n",
    "                    elif len(concerns1) == 0 or len(concerns2) == 0:\n",
    "                        agreement = 0.0  # One has concerns, other doesn't\n",
    "                    else:\n",
    "                        # Jaccard similarity\n",
    "                        intersection = len(concerns1.intersection(concerns2))\n",
    "                        union = len(concerns1.union(concerns2))\n",
    "                        agreement = intersection / union if union > 0 else 0\n",
    "                    \n",
    "                    agreement_scores.append(agreement)\n",
    "        \n",
    "        if agreement_scores:\n",
    "            avg_agreement = np.mean(agreement_scores)\n",
    "            print(f\"Average concern agreement (Jaccard): {avg_agreement:.3f}\")\n",
    "            print(f\"Agreement range: {min(agreement_scores):.3f} - {max(agreement_scores):.3f}\")\n",
    "    \n",
    "    return overall_concern_counts, expert_concern_counts, concern_pairs\n",
    "\n",
    "def analyze_yesno_patterns(all_data, mapping_df=None):\n",
    "    \"\"\"Analyze Yes/No question patterns\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ YES/NO QUESTION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    yesno_fields = ['no_text', 'well_separated_obj']\n",
    "    \n",
    "    for field in yesno_fields:\n",
    "        print(f\"\\nüìä {field.upper().replace('_', ' ')} ANALYSIS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Overall response distribution\n",
    "        field_responses = defaultdict(Counter)\n",
    "        all_responses = []\n",
    "        \n",
    "        for expert_name, expert_data in all_data.items():\n",
    "            for record in expert_data:\n",
    "                response = record[field]\n",
    "                if response:\n",
    "                    field_responses[expert_name][response] += 1\n",
    "                    all_responses.append(response)\n",
    "        \n",
    "        # Overall distribution\n",
    "        overall_counts = Counter(all_responses)\n",
    "        total_responses = len(all_responses)\n",
    "        \n",
    "        print(\"Overall distribution:\")\n",
    "        for response, count in overall_counts.items():\n",
    "            percentage = (count / total_responses) * 100\n",
    "            print(f\"  {response}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # By expert\n",
    "        print(f\"\\nDistribution by expert:\")\n",
    "        print(f\"{'Expert':<10} {'Yes':>6} {'No':>6} {'Yes %':>8}\")\n",
    "        print(\"-\" * 32)\n",
    "        \n",
    "        for expert_name in ANNOTATION_FILES.keys():\n",
    "            yes_count = field_responses[expert_name]['Yes']\n",
    "            no_count = field_responses[expert_name]['No']\n",
    "            total_expert = yes_count + no_count\n",
    "            yes_pct = (yes_count / total_expert * 100) if total_expert > 0 else 0\n",
    "            \n",
    "            print(f\"{expert_name:<10} {yes_count:6d} {no_count:6d} {yes_pct:7.1f}%\")\n",
    "        \n",
    "        # Expert agreement analysis\n",
    "        print(f\"\\nü§ù Expert Agreement for {field.replace('_', ' ').title()}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Group responses by image\n",
    "        image_responses = defaultdict(list)\n",
    "        for expert_name, expert_data in all_data.items():\n",
    "            for record in expert_data:\n",
    "                if record[field]:\n",
    "                    image_responses[record['image']].append({\n",
    "                        'expert': expert_name,\n",
    "                        'response': record[field]\n",
    "                    })\n",
    "        \n",
    "        # Calculate agreement for images with multiple annotations\n",
    "        multi_expert_images = {img: responses for img, responses in image_responses.items() if len(responses) > 1}\n",
    "        \n",
    "        if multi_expert_images:\n",
    "            agreement_count = 0\n",
    "            total_comparisons = 0\n",
    "            \n",
    "            for img, responses in multi_expert_images.items():\n",
    "                # Check if all experts agree\n",
    "                unique_responses = set(r['response'] for r in responses)\n",
    "                if len(unique_responses) == 1:\n",
    "                    agreement_count += 1\n",
    "                total_comparisons += 1\n",
    "            \n",
    "            agreement_rate = (agreement_count / total_comparisons) * 100 if total_comparisons > 0 else 0\n",
    "            print(f\"Images with multiple annotations: {total_comparisons}\")\n",
    "            print(f\"Perfect agreement: {agreement_count}/{total_comparisons} ({agreement_rate:.1f}%)\")\n",
    "        \n",
    "        # Style-specific analysis (if mapping provided)\n",
    "        if mapping_df is not None:\n",
    "            print(f\"\\nüé® {field.replace('_', ' ').title()} by Style\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Create image to style mapping\n",
    "            style_mapping = dict(zip(mapping_df['new_filename'], mapping_df['style']))\n",
    "            \n",
    "            style_responses = defaultdict(Counter)\n",
    "            \n",
    "            for expert_name, expert_data in all_data.items():\n",
    "                for record in expert_data:\n",
    "                    image_file = record['image']\n",
    "                    if image_file in style_mapping and record[field]:\n",
    "                        style = style_mapping[image_file]\n",
    "                        style_responses[style][record[field]] += 1\n",
    "            \n",
    "            # Display style-specific patterns\n",
    "            for style in sorted(style_responses.keys()):\n",
    "                yes_count = style_responses[style]['Yes']\n",
    "                no_count = style_responses[style]['No']\n",
    "                total_style = yes_count + no_count\n",
    "                yes_pct = (yes_count / total_style * 100) if total_style > 0 else 0\n",
    "                \n",
    "                print(f\"{style:<15}: {yes_count:3d} Yes, {no_count:3d} No ({yes_pct:5.1f}% Yes)\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main analysis function\"\"\"\n",
    "    print(\"üöÄ Starting Concern Pattern and Yes/No Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load all data\n",
    "    all_data = {}\n",
    "    for expert_name, filename in ANNOTATION_FILES.items():\n",
    "        print(f\"Loading {expert_name} data...\")\n",
    "        annotations = load_annotations(expert_name, filename)\n",
    "        all_data[expert_name] = extract_structured_data(annotations, expert_name)\n",
    "        print(f\"  Extracted data for {len(all_data[expert_name])} images\")\n",
    "    \n",
    "    # Run analyses\n",
    "    overall_concern_counts, expert_concern_counts, concern_pairs = analyze_concern_patterns(all_data)\n",
    "    \n",
    "    # Try to load mapping file for style analysis\n",
    "    mapping_df = None\n",
    "    try:\n",
    "        # Assuming you have a mapping file - adjust path as needed\n",
    "        mapping_df = pd.read_csv('path_to_your_mapping_file.csv')\n",
    "        print(\"‚úÖ Mapping file loaded for style analysis\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  No mapping file found - skipping style-specific analysis\")\n",
    "    \n",
    "    analyze_yesno_patterns(all_data, mapping_df)\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_visualizations(overall_concern_counts, expert_concern_counts, all_data)\n",
    "    \n",
    "    print(\"\\nüéâ Analysis complete!\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
